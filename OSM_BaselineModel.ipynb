{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoIkRSPEKAhj",
        "outputId": "81db5b12-94f1-48e9-a025-5b5730b959a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading and Preprocessing Data ---\n",
            "Original dataset shape: (20000, 11)\n",
            "Dataset shape after dropping NaNs: (20000, 11)\n",
            "Training dataset size: 16000\n",
            "Evaluation dataset size: 4000\n",
            "\n",
            "--- Starting Training (TF-IDF + Logistic Regression) ---\n",
            "Training complete.\n",
            "\n",
            "--- Final Evaluation Results ---\n",
            "Accuracy: 0.8598\n",
            "F1 Score: 0.8597\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Depression       0.86      0.86      0.86      2000\n",
            "   Depression       0.86      0.86      0.86      2000\n",
            "\n",
            "     accuracy                           0.86      4000\n",
            "    macro avg       0.86      0.86      0.86      4000\n",
            " weighted avg       0.86      0.86      0.86      4000\n",
            "\n",
            "\n",
            "--- Running Inference on New Data ---\n",
            "\n",
            "Text: The estate agent is officially pissing me off now...\n",
            "Prediction: Depression (Score: 0.5764)\n",
            "\n",
            "Text: Ahh. Foundations was fun.\n",
            "Prediction: Depression (Score: 0.5346)\n",
            "\n",
            "Text: Some people find themselves turning to colleagues to vent their #frustrations.\n",
            "Prediction: No Depression (Score: 0.5512)\n",
            "\n",
            "Text: True love is basically me holding Andy for the first time. I can still remember opening the box like it was just last October.\n",
            "Prediction: No Depression (Score: 0.5234)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ==============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "print(\"\\n--- Loading and Preprocessing Data ---\")\n",
        "\n",
        "# ==============================================\n",
        "# 2. DATA LOADING AND PREPROCESSING\n",
        "# ==============================================\n",
        "try:\n",
        "    df = pd.read_csv(\"Mental-Health-Twitter.csv\")\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Mental-Health-Twitter.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Filter out rows where 'post_text' or 'label' might be missing\n",
        "df.dropna(subset=['post_text', 'label'], inplace=True)\n",
        "print(f\"Dataset shape after dropping NaNs: {df.shape}\")\n",
        "\n",
        "# Ensure 'label' column is integer\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "# Map labels for display later\n",
        "id2label = {0: \"No Depression\", 1: \"Depression\"}\n",
        "\n",
        "# Define X (Features) and y (Target)\n",
        "X = df['post_text']\n",
        "y = df['label']\n",
        "\n",
        "# ==============================================\n",
        "# 3. TRAIN / TEST SPLIT\n",
        "# ==============================================\n",
        "# Splitting 80% for training and 20% for evaluation (same ratio as your Roberta code)\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training dataset size: {len(X_train)}\")\n",
        "print(f\"Evaluation dataset size: {len(X_eval)}\")\n",
        "\n",
        "# ==============================================\n",
        "# 4. PIPELINE SETUP (TF-IDF + LOGISTIC REGRESSION)\n",
        "# ==============================================\n",
        "# We create a pipeline to bundle the pre-processing and model together.\n",
        "# 1. TfidfVectorizer: Converts text to numerical vectors based on word frequency.\n",
        "# 2. LogisticRegression: The classifier.\n",
        "\n",
        "model_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        stop_words='english',  # Remove common words like 'the', 'is', 'and'\n",
        "        max_features=5000,     # Keep only top 5000 most frequent words to reduce noise\n",
        "        ngram_range=(1, 2)     # Look at unigrams (words) and bigrams (two-word pairs)\n",
        "    )),\n",
        "    ('clf', LogisticRegression(\n",
        "        class_weight='balanced', # Handle imbalance if one class is much larger than the other\n",
        "        solver='liblinear',      # Good solver for smaller datasets or high dimensionality\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# ==============================================\n",
        "# 5. MODEL TRAINING\n",
        "# ==============================================\n",
        "print(f\"\\n--- Starting Training (TF-IDF + Logistic Regression) ---\")\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# ==============================================\n",
        "# 6. EVALUATION\n",
        "# ==============================================\n",
        "print(\"\\n--- Final Evaluation Results ---\")\n",
        "\n",
        "# Generate predictions on the evaluation set\n",
        "y_pred = model_pipeline.predict(X_eval)\n",
        "\n",
        "# Calculate Metrics\n",
        "accuracy = accuracy_score(y_eval, y_pred)\n",
        "f1 = f1_score(y_eval, y_pred, average='weighted') # 'binary' if strictly 2 classes, 'weighted' handles general cases\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_eval, y_pred, target_names=[\"No Depression\", \"Depression\"]))\n",
        "\n",
        "# ==============================================\n",
        "# 7. INFERENCE (TESTING ON NEW DATA)\n",
        "# ==============================================\n",
        "new_data = [\n",
        "    \"The estate agent is officially pissing me off now...\",\n",
        "    \"Ahh. Foundations was fun.\",\n",
        "    \"Some people find themselves turning to colleagues to vent their #frustrations.\",\n",
        "    \"True love is basically me holding Andy for the first time. I can still remember opening the box like it was just last October.\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Running Inference on New Data ---\")\n",
        "\n",
        "# The pipeline handles vectorization automatically for new data\n",
        "predictions = model_pipeline.predict(new_data)\n",
        "probs = model_pipeline.predict_proba(new_data) # Get confidence scores\n",
        "\n",
        "for text, pred, prob in zip(new_data, predictions, probs):\n",
        "    label_name = id2label[pred]\n",
        "    confidence = prob[pred] # Score of the predicted class\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {label_name} (Score: {confidence:.4f})\")"
      ]
    }
  ]
}